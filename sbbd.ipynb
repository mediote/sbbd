{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a45be2",
   "metadata": {
    "id": "b8a45be2"
   },
   "source": [
    "# 1-  Data crawling on TwitterAPI: Full-archive search \n",
    "\n",
    "Documentation: https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all \n",
    "\n",
    "Endpoint URL: https://api.twitter.com/2/tweets/search/all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e68fd6",
   "metadata": {
    "id": "54e68fd6"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import urllib.request as urllib2\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "auth_token = os.environ.get('AUTH_TOKEN')\n",
    "header = {'Authorization': 'Bearer ' + auth_token}\n",
    "\n",
    "class TwitterHook():\n",
    "\n",
    "    def __init__(self, query, header = None, start_time = None, end_time = None, max_results= None):\n",
    "        self.query = query\n",
    "        self.header = header\n",
    "        self.start_time = '2020-02-29T00%3A00%3A00Z'\n",
    "        self.end_time = '2021-05-04T00%3A00%3A00Z'\n",
    "        self.max_results = '500'\n",
    "\n",
    "    def create_url(self):\n",
    "        query = self.query\n",
    "        start_time = self.start_time\n",
    "        end_time = self.end_time        \n",
    "        \n",
    "        tweet_fields = \"tweet.fields=author_id,id,created_at,in_reply_to_user_id,text\"\n",
    "        user_fields = \"expansions=author_id&user.fields=id,name,username,created_at\"\n",
    "        start_time = (\n",
    "            f\"&start_time={self.start_time}\"\n",
    "            if self.start_time\n",
    "            else \"\"\n",
    "        )\n",
    "        end_time = (\n",
    "            f\"&end_time={self.end_time}\"\n",
    "            if self.end_time\n",
    "            else \"\"\n",
    "        )\n",
    "        max_results  = (\n",
    "            f\"&max_results={self.max_results}\"\n",
    "            if self.max_results\n",
    "            else \"\"\n",
    "        )\n",
    "        url = \"https://api.twitter.com/2/tweets/search/all?query={}&{}&{}{}{}{}\".format(\n",
    "               query, tweet_fields, user_fields, start_time, end_time, max_results\n",
    "        )\n",
    "        return url\n",
    "\n",
    "    def connect_to_endpoint(self, url, header):\n",
    "        response = requests.get(url,headers=header)\n",
    "        listOfTweets = json.loads(response.content)\n",
    "        return  listOfTweets\n",
    "\n",
    "\n",
    "    def paginate(self, url, header, next_token=\"\"):\n",
    "        if next_token:\n",
    "            full_url = f\"{url}&next_token={next_token}\"\n",
    "            print('New Request on',full_url)\n",
    "        else:\n",
    "            full_url = url\n",
    "            print('New Request on',full_url)\n",
    "        data = self.connect_to_endpoint(full_url, header)\n",
    "        yield data\n",
    "        if \"next_token\" in data.get(\"meta\", {}):\n",
    "            yield from self.paginate(url, header, data['meta']['next_token'])\n",
    "\n",
    "\n",
    "    def run(self):  \n",
    "        url = self.create_url()\n",
    "        yield from self.paginate(url, header)\n",
    "        \n",
    "        \n",
    "def GetTweets(query):\n",
    "    tweets = pd.DataFrame()\n",
    "    for pg in TwitterHook(query).run():\n",
    "        time.sleep(1)  \n",
    "        \n",
    "        if 'data' in pg:\n",
    "            tweets =  tweets.append(pg['data'],ignore_index=True)\n",
    "        else:\n",
    "             print('Missing request')\n",
    "        \n",
    "    print('Done! Total of', len(tweets), 'tweets collected.')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a16896",
   "metadata": {
    "id": "61a16896",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = GetTweets(urllib2.quote('#vacinanao -rt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc6800",
   "metadata": {
    "id": "8ecc6800"
   },
   "outputs": [],
   "source": [
    "#tweets.to_csv('./vacinaobrigatorianao.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222cf05",
   "metadata": {
    "id": "a222cf05"
   },
   "source": [
    "# 2- Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66004b1c",
   "metadata": {},
   "source": [
    "### Import datasets from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aPg7p1mOUht",
   "metadata": {
    "id": "2aPg7p1mOUht"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id':\"1kzlbUH76yT_J4sMj8ZMf0ZsyrpNK0uhX\"}) \n",
    "downloaded.GetContentFile('provaxxersProcessed.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "provaxxers =  pd.read_csv('./provaxxersProcessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741672d",
   "metadata": {},
   "source": [
    "### Import datasets from local disck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47550598",
   "metadata": {
    "id": "47550598"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "antivaxxers = pd.read_csv('./datasets/antivaxxersTweets.csv', low_memory=False)\n",
    "provaxxers = pd.read_csv('./datasets/provaxxersTweets.csv', low_memory=False)\n",
    "\n",
    "start_date ='2020-03-11T00:59:59.000Z'\n",
    "end_date = '2022-04-06T00:00:00.000Z'\n",
    "\n",
    "mask_provaxxers = (provaxxers['created_at'] >= start_date) & (provaxxers['created_at'] <= end_date)\n",
    "provaxxers = provaxxers.loc[mask_provaxxers]\n",
    "\n",
    "mask_antivaxxers = (antivaxxers['created_at'] >= start_date) & (antivaxxers['created_at'] <= end_date)\n",
    "antivaxxers = antivaxxers.loc[mask_antivaxxers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8734080d",
   "metadata": {},
   "source": [
    "### Pre-proccess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae16ca83",
   "metadata": {
    "id": "ae16ca83"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "import unidecode\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "\n",
    "def proccess_text(tweets):\n",
    "    \n",
    "    # Removing links, mentions and hashtags\n",
    "    tweets['processed_text'] = tweets.text.str.replace(r'(http\\S+)', '',regex=True) \\\n",
    "                                          .str.replace(r'@[\\w]*', '',regex=True) \\\n",
    "                                          .str.replace(r'#[\\w]*','',regex=True) \n",
    "    print('[ok] - Removing links.')\n",
    "    print('[ok] - Removing mentions.')\n",
    "    print('[ok] - Removing hashtags.')\n",
    "\n",
    "    textWords = ' '.join([text for text in tweets.processed_text])\n",
    "\n",
    "    # Removing accent\n",
    "    textWords = [unidecode.unidecode(text) for text in tweets.processed_text ]    \n",
    "    print('[ok] - Removing accent.')\n",
    "    \n",
    "    # Creating a list of words and characters (stopwords) to be removed from the text\n",
    "    # stopWords = nltk.corpus.stopwords.words(\"portuguese\")    \n",
    "    print('[ok] - Creating a list of words and characters (stopwords) to be removed from the text.')\n",
    "    \n",
    "    \n",
    "    # Separating punctuation from words\n",
    "    punctSeparator = tokenize.WordPunctTokenizer()\n",
    "    punctuationList = list()\n",
    "    for punct in punctuation:\n",
    "        punctuationList.append(punct)\n",
    "        \n",
    "    #stopWords =   punctuationList + stopWords    \n",
    "    stopWords =   punctuationList\n",
    "    #print('[ok] - Separating punctuation from words.')\n",
    "\n",
    "\n",
    "    # Iterating over the text and removing stop words \n",
    "    trasnformedText = list()    \n",
    "    for text in textWords:\n",
    "        newText = list()   \n",
    "        text = text.lower()\n",
    "        textWords = punctSeparator.tokenize(text)\n",
    "        for words in textWords:\n",
    "             if words not in stopWords:\n",
    "                #newText.append(stemmer.stem(words))\n",
    "                newText.append(words)\n",
    "        trasnformedText.append(' '.join(newText))\n",
    "    tweets.processed_text = trasnformedText\n",
    "    print('[ok] - Removing punctuation and set text to lowecase.')\n",
    "   \n",
    "    # Removing all non-text characters\n",
    "    tweets.processed_text = tweets['processed_text'].str.replace(r\"[^a-zA-Z#]\", \" \", regex=True)                                                         \n",
    "    print('[ok] - Removing all non-text characters.')\n",
    "   \n",
    "    trasnformedText = list()\n",
    "    for phrase in tweets.processed_text:\n",
    "        newPhrase = list()   \n",
    "        newPhrase.append(' '.join(phrase.split()))\n",
    "        for words in newPhrase:\n",
    "            trasnformedText.append(''.join(newPhrase))\n",
    "    tweets.processed_text = trasnformedText\n",
    "    \n",
    "    # Removing tweets with less than three terms\n",
    "    index=[x for x in tweets.index if tweets.processed_text[x].count(' ') < 3]\n",
    "    tweets = tweets.drop(index)\n",
    "    print('[ok] - Removing tweets with less than three terms.')\n",
    "\n",
    "    # Removing empty lines\n",
    "    removeEmpty  = tweets.processed_text != ' '\n",
    "    tweets = tweets[removeEmpty]\n",
    "    print('[ok] - Removing empty lines.')\n",
    "\n",
    "    tweets.reset_index(inplace=True)\n",
    "    tweets = {'created_at': tweets.created_at, 'id':tweets.id,'author_id':tweets.author_id,'in_reply_to_user_id':tweets.in_reply_to_user_id, 'text': tweets.processed_text}\n",
    "    tweets = pd.DataFrame(tweets)\n",
    "    tweets = tweets.sort_values(['created_at']).reset_index().drop(columns=[\"index\"])\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c759d176",
   "metadata": {
    "id": "c759d176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] - Removing links.\n",
      "[ok] - Removing mentions.\n",
      "[ok] - Removing hashtags.\n",
      "[ok] - Removing accent.\n",
      "[ok] - Creating a list of words and characters (stopwords) to be removed from the text.\n",
      "[ok] - Removing punctuation and set texto to lowecase.\n",
      "[ok] - Removing all non-text characters.\n",
      "[ok] - Removing tweets with less than three terms.\n",
      "[ok] - Removing empty lines.\n"
     ]
    }
   ],
   "source": [
    "#provaxxers = proccess_text(provaxxers)\n",
    "antivaxxers  = proccess_text(antivaxxers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ca3d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provaxxers.to_csv('./datasets/provaxxersProcessed.csv',index=False)\n",
    "antivaxxers.to_csv('./datasets/antivaxxersProcessed.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2cf6c",
   "metadata": {
    "id": "3bd2cf6c"
   },
   "source": [
    "# 3- Topic Modeling with BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6509094",
   "metadata": {
    "id": "e6509094"
   },
   "source": [
    "### Checking GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5968c368",
   "metadata": {
    "id": "5968c368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():        \n",
    "    device = torch.device(\"cuda\")    \n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c25c68ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 12 16:16:04 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 462.31       Driver Version: 462.31       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 105... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   45C    P8    N/A /  N/A |    266MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1684    C+G   ...erver\\YourPhoneServer.exe    N/A      |\n",
      "|    0   N/A  N/A     12256    C+G   Insufficient Permissions        N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2d4ce",
   "metadata": {},
   "source": [
    "### Enable RAPIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4314a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'rapidsai-csp-utils'...\n",
      "Traceback (most recent call last):\n",
      "  File \"rapidsai-csp-utils/colab/env-check.py\", line 1, in <module>\n",
      "    import pynvml\n",
      "ModuleNotFoundError: No module named 'pynvml'\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!python rapidsai-csp-utils/colab/env-check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b59d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bash rapidsai-csp-utils/colab/update_gcc.sh\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7330c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import condacolab\n",
    "condacolab.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import condacolab\n",
    "condacolab.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python rapidsai-csp-utils/colab/install_rapids.py stable\n",
    "import os\n",
    "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
    "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
    "os.environ['CONDA_PREFIX'] = '/usr/local'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757b7ac",
   "metadata": {},
   "source": [
    "### Checking RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f160a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 17.0 gigabytes of available RAM\n",
      "\n",
      "Not using a high-RAM runtime\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740262a5",
   "metadata": {},
   "source": [
    "### Custom BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb74a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f76a6",
   "metadata": {},
   "source": [
    "### Custom UMAP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b77de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.manifold import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba33ee73",
   "metadata": {},
   "source": [
    "### Custom HDBSCAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import HDBSCAN\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf5070",
   "metadata": {},
   "source": [
    "### Custom vectorizer model model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8e376",
   "metadata": {
    "id": "cda8e376"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(2, 2), stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c58db",
   "metadata": {},
   "source": [
    "### Initializing BERTopic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a980c25",
   "metadata": {
    "id": "9a980c25"
   },
   "outputs": [],
   "source": [
    "docs = tweets\n",
    "\n",
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic(#language = 'english',\n",
    "                       embedding_model=bert_model,\n",
    "                       top_n_words=10,\n",
    "                       #n_gram_range=(1, 2),\n",
    "                       min_topic_size=50,   \n",
    "                       nr_topics = 'auto',\n",
    "                       #umap_model=umap_model,  \n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       low_memory=True,\n",
    "                       calculate_probabilities=False, \n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4799676e",
   "metadata": {
    "id": "4799676e"
   },
   "source": [
    "### Generating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5825bfd9",
   "metadata": {
    "id": "5825bfd9"
   },
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(docs.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e928d",
   "metadata": {
    "id": "1f7e928d"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f147bfe",
   "metadata": {
    "id": "4f147bfe"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b39ab",
   "metadata": {
    "id": "1e2b39ab"
   },
   "source": [
    "### Reducing the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df7dce",
   "metadata": {
    "id": "31df7dce"
   },
   "outputs": [],
   "source": [
    "newTopics, newProbs = topic_model.reduce_topics(docs.text, topics, probs, nr_topics=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392913af",
   "metadata": {
    "id": "392913af"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa33064",
   "metadata": {
    "id": "8fa33064"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264570f",
   "metadata": {
    "id": "1264570f"
   },
   "outputs": [],
   "source": [
    "topic_model.get_representative_docs(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073e5ab",
   "metadata": {
    "id": "a073e5ab"
   },
   "source": [
    "### Dynamic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848f875",
   "metadata": {
    "id": "6848f875"
   },
   "outputs": [],
   "source": [
    "timestamps = docs.created_at.to_list()\n",
    "tweets = docs.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f79ee",
   "metadata": {
    "id": "779f79ee"
   },
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(docs=tweets, \n",
    "                                                topics=newTopics,                                                                                           \n",
    "                                                timestamps=timestamps, \n",
    "                                                global_tuning=True,\n",
    "                                                evolution_tuning=True, \n",
    "                                                nr_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca25ac",
   "metadata": {
    "id": "45ca25ac"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f568f0",
   "metadata": {
    "id": "75f568f0"
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sbbd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
