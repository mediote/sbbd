{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a45be2",
   "metadata": {
    "id": "b8a45be2"
   },
   "source": [
    "# 1-  Data crawling on TwitterAPI: Full-archive search \n",
    "\n",
    "Documentation: https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all \n",
    "\n",
    "Endpoint URL: https://api.twitter.com/2/tweets/search/all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e68fd6",
   "metadata": {
    "id": "54e68fd6"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import urllib.request as urllib2\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "auth_token = os.environ.get('AUTH_TOKEN')\n",
    "header = {'Authorization': 'Bearer ' + auth_token}\n",
    "\n",
    "class TwitterHook():\n",
    "\n",
    "    def __init__(self, query, header = None, start_time = None, end_time = None, max_results= None):\n",
    "        self.query = query\n",
    "        self.header = header\n",
    "        self.start_time = '2020-02-29T00%3A00%3A00Z'\n",
    "        self.end_time = '2021-05-04T00%3A00%3A00Z'\n",
    "        self.max_results = '500'\n",
    "\n",
    "    def create_url(self):\n",
    "        query = self.query\n",
    "        start_time = self.start_time\n",
    "        end_time = self.end_time        \n",
    "        \n",
    "        tweet_fields = \"tweet.fields=author_id,id,created_at,in_reply_to_user_id,text\"\n",
    "        user_fields = \"expansions=author_id&user.fields=id,name,username,created_at\"\n",
    "        start_time = (\n",
    "            f\"&start_time={self.start_time}\"\n",
    "            if self.start_time\n",
    "            else \"\"\n",
    "        )\n",
    "        end_time = (\n",
    "            f\"&end_time={self.end_time}\"\n",
    "            if self.end_time\n",
    "            else \"\"\n",
    "        )\n",
    "        max_results  = (\n",
    "            f\"&max_results={self.max_results}\"\n",
    "            if self.max_results\n",
    "            else \"\"\n",
    "        )\n",
    "        url = \"https://api.twitter.com/2/tweets/search/all?query={}&{}&{}{}{}{}\".format(\n",
    "               query, tweet_fields, user_fields, start_time, end_time, max_results\n",
    "        )\n",
    "        return url\n",
    "\n",
    "    def connect_to_endpoint(self, url, header):\n",
    "        response = requests.get(url,headers=header)\n",
    "        listOfTweets = json.loads(response.content)\n",
    "        return  listOfTweets\n",
    "\n",
    "\n",
    "    def paginate(self, url, header, next_token=\"\"):\n",
    "        if next_token:\n",
    "            full_url = f\"{url}&next_token={next_token}\"\n",
    "            print('New Request on',full_url)\n",
    "        else:\n",
    "            full_url = url\n",
    "            print('New Request on',full_url)\n",
    "        data = self.connect_to_endpoint(full_url, header)\n",
    "        yield data\n",
    "        if \"next_token\" in data.get(\"meta\", {}):\n",
    "            yield from self.paginate(url, header, data['meta']['next_token'])\n",
    "\n",
    "\n",
    "    def run(self):  \n",
    "        url = self.create_url()\n",
    "        yield from self.paginate(url, header)\n",
    "        \n",
    "        \n",
    "def GetTweets(query):\n",
    "    tweets = pd.DataFrame()\n",
    "    for pg in TwitterHook(query).run():\n",
    "        time.sleep(1)  \n",
    "        \n",
    "        if 'data' in pg:\n",
    "            tweets =  tweets.append(pg['data'],ignore_index=True)\n",
    "        else:\n",
    "             print('Missing request')\n",
    "        \n",
    "    print('Done! Total of', len(tweets), 'tweets collected.')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a16896",
   "metadata": {
    "id": "61a16896",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = GetTweets(urllib2.quote('#vacinanao -rt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222cf05",
   "metadata": {
    "id": "a222cf05"
   },
   "source": [
    "# 2- Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66004b1c",
   "metadata": {},
   "source": [
    "### 2.1- Import datasets from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aPg7p1mOUht",
   "metadata": {
    "id": "2aPg7p1mOUht"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "tweets =  pd.read_csv('/content/drive/MyDrive/datasets/provaxxers/your_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741672d",
   "metadata": {},
   "source": [
    "### 2.2- Import datasets from local disck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47550598",
   "metadata": {
    "id": "47550598"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "antivaxxers = pd.read_csv('./yourdirectory/antivaxxersTweets.csv', low_memory=False)\n",
    "provaxxers = pd.read_csv('./yourdirectory/provaxxersTweets.csv', low_memory=False)\n",
    "\n",
    "start_date ='2020-03-11T00:59:59.000Z'\n",
    "end_date = '2022-04-06T00:00:00.000Z'\n",
    "\n",
    "mask_provaxxers = (provaxxers['created_at'] >= start_date) & (provaxxers['created_at'] <= end_date)\n",
    "provaxxers = provaxxers.loc[mask_provaxxers]\n",
    "\n",
    "mask_antivaxxers = (antivaxxers['created_at'] >= start_date) & (antivaxxers['created_at'] <= end_date)\n",
    "antivaxxers = antivaxxers.loc[mask_antivaxxers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8734080d",
   "metadata": {},
   "source": [
    "### 2.3- Pre-proccess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae16ca83",
   "metadata": {
    "id": "ae16ca83"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "import unidecode\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "\n",
    "def proccess_text(tweets):\n",
    "    \n",
    "    # Removing links, mentions and hashtags\n",
    "    tweets['processed_text'] = tweets.text.str.replace(r'(http\\S+)', '',regex=True) \\\n",
    "                                          .str.replace(r'@[\\w]*', '',regex=True) \\\n",
    "                                          .str.replace(r'#[\\w]*','',regex=True) \n",
    "    print('[ok] - Removing links.')\n",
    "    print('[ok] - Removing mentions.')\n",
    "    print('[ok] - Removing hashtags.')\n",
    "\n",
    "    textWords = ' '.join([text for text in tweets.processed_text])\n",
    "\n",
    "    # Removing accent\n",
    "    textWords = [unidecode.unidecode(text) for text in tweets.processed_text ]    \n",
    "    print('[ok] - Removing accent.')\n",
    "    \n",
    "    # Creating a list of words and characters (stopwords) to be removed from the text\n",
    "    # stopWords = nltk.corpus.stopwords.words(\"portuguese\")    \n",
    "    print('[ok] - Creating a list of words and characters (stopwords) to be removed from the text.')\n",
    "    \n",
    "    \n",
    "    # Separating punctuation from words\n",
    "    punctSeparator = tokenize.WordPunctTokenizer()\n",
    "    punctuationList = list()\n",
    "    for punct in punctuation:\n",
    "        punctuationList.append(punct)\n",
    "        \n",
    "    #stopWords =   punctuationList + stopWords    \n",
    "    stopWords =   punctuationList\n",
    "    #print('[ok] - Separating punctuation from words.')\n",
    "\n",
    "\n",
    "    # Iterating over the text and removing stop words \n",
    "    trasnformedText = list()    \n",
    "    for text in textWords:\n",
    "        newText = list()   \n",
    "        text = text.lower()\n",
    "        textWords = punctSeparator.tokenize(text)\n",
    "        for words in textWords:\n",
    "             if words not in stopWords:\n",
    "                #newText.append(stemmer.stem(words))\n",
    "                newText.append(words)\n",
    "        trasnformedText.append(' '.join(newText))\n",
    "    tweets.processed_text = trasnformedText\n",
    "    print('[ok] - Removing punctuation and set text to lowecase.')\n",
    "   \n",
    "    # Removing all non-text characters\n",
    "    tweets.processed_text = tweets['processed_text'].str.replace(r\"[^a-zA-Z#]\", \" \", regex=True)                                                         \n",
    "    print('[ok] - Removing all non-text characters.')\n",
    "   \n",
    "    trasnformedText = list()\n",
    "    for phrase in tweets.processed_text:\n",
    "        newPhrase = list()   \n",
    "        newPhrase.append(' '.join(phrase.split()))\n",
    "        for words in newPhrase:\n",
    "            trasnformedText.append(''.join(newPhrase))\n",
    "    tweets.processed_text = trasnformedText\n",
    "    \n",
    "    # Removing tweets with less than three terms\n",
    "    index=[x for x in tweets.index if tweets.processed_text[x].count(' ') < 3]\n",
    "    tweets = tweets.drop(index)\n",
    "    print('[ok] - Removing tweets with less than three terms.')\n",
    "\n",
    "    # Removing empty lines\n",
    "    removeEmpty  = tweets.processed_text != ' '\n",
    "    tweets = tweets[removeEmpty]\n",
    "    print('[ok] - Removing empty lines.')\n",
    "\n",
    "    tweets.reset_index(inplace=True)\n",
    "    tweets = {'created_at': tweets.created_at, 'id':tweets.id,'author_id':tweets.author_id,'in_reply_to_user_id':tweets.in_reply_to_user_id, 'text': tweets.processed_text}\n",
    "    #tweets = {'text': tweets.processed_text,'stance':tweets.stance}\n",
    "    tweets = pd.DataFrame(tweets)\n",
    "    tweets = tweets.sort_values(['created_at']).reset_index().drop(columns=[\"index\"])\n",
    "    #tweets = tweets.reset_index().drop(columns=[\"index\"])\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c759d176",
   "metadata": {
    "id": "c759d176"
   },
   "outputs": [],
   "source": [
    "provaxxers = proccess_text(provaxxers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2cf6c",
   "metadata": {
    "id": "3bd2cf6c"
   },
   "source": [
    "# 3- Topic Modeling with BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf316e",
   "metadata": {},
   "source": [
    "### 3.1- Load pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57cb5d99",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5948/741742535.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive/MyDrive/datasets/provaxxers/provaxxers_processed.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "tweets =  pd.read_csv('/content/drive/MyDrive/datasets/provaxxers/provaxxers_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2c4d9",
   "metadata": {},
   "source": [
    "### 3.2- Checking dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'bertopic' not in sys.modules:\n",
    "    print('Installing requeriment..')\n",
    "    ! pip install bertopic\n",
    "else:\n",
    "    print('Requirement already satisfied..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6509094",
   "metadata": {
    "id": "e6509094"
   },
   "source": [
    "### 3.3- Checking GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968c368",
   "metadata": {
    "id": "5968c368"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():        \n",
    "    device = torch.device(\"cuda\")    \n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c68ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757b7ac",
   "metadata": {},
   "source": [
    "### 3.4- Checking RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f160a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740262a5",
   "metadata": {},
   "source": [
    "### 3.5- Custom Embbedings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb74a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "bert_model = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58202eb4",
   "metadata": {},
   "source": [
    "### 3.6- Custom UMAP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095cc7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba33ee73",
   "metadata": {},
   "source": [
    "### 3.7- Custom HDBSCAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size = 65,\n",
    "                                metric='euclidean', \n",
    "                                cluster_selection_method='eom')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf5070",
   "metadata": {},
   "source": [
    "### 3.8- Custom vectorizer model model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8e376",
   "metadata": {
    "id": "cda8e376"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(2, 2), stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c58db",
   "metadata": {},
   "source": [
    "### 3.9- Initializing BERTopic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a980c25",
   "metadata": {
    "id": "9a980c25"
   },
   "outputs": [],
   "source": [
    "docs = tweets\n",
    "\n",
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic(#language = 'english',\n",
    "                       embedding_model=bert_model,\n",
    "                       top_n_words=10,\n",
    "                       #n_gram_range=(1, 2),\n",
    "                       #min_topic_size=50,   \n",
    "                       nr_topics = 'auto',\n",
    "                       umap_model=umap_model,\n",
    "                       hdbscan_model=hdbscan_model\n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       low_memory=True,\n",
    "                       calculate_probabilities=False, \n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4799676e",
   "metadata": {
    "id": "4799676e"
   },
   "source": [
    "### 3.10- Generating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5825bfd9",
   "metadata": {
    "id": "5825bfd9"
   },
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(docs.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e2903",
   "metadata": {},
   "source": [
    "### 3.11- Serialize models, topics and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4999633",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(\"/content/drive/MyDrive/datasets/models/model_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f3e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = pd.DataFrame(topics, columns=['topic'])\n",
    "topic.to_csv('/content/drive/MyDrive/datasets/models/provaxxers_all_mpnet_base_v2_topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c46f3b",
   "metadata": {},
   "source": [
    "### 3.12- Load models,topics and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feadfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "if 'bertopic' not in sys.modules:\n",
    "    print('Installing requeriment..')\n",
    "    ! pip install bertopic\n",
    "    from bertopic import BERTopic\n",
    "else:\n",
    "    from bertopic import BERTopic\n",
    "    print('Requirement already satisfied..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "topic_model = BERTopic.load(\"/content/drive/MyDrive/datasets/models/provaxxers_all_mpnet_base_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376357e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics= pd.read_csv('/content/drive/MyDrive/datasets/models/provaxxers_all_mpnet_base_v2_topics.csv')\n",
    "topics=topics.drop(columns=[\"Unnamed: 0\"])\n",
    "topics = topics.topic\n",
    "\n",
    "docs = pd.read_csv('/content/drive/MyDrive/datasets/provaxxers/provaxxers_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc703f4",
   "metadata": {},
   "source": [
    "### 3.13- Inspect topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96143dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs = {topic: [] for topic in set(topics)}\n",
    "for topic, doc in zip(topics, docs.text):\n",
    "    topic_docs[topic].append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0373414",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.find_topics(\"canada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a76449",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_representative_docs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a51f60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#provaxxers\n",
    "t_values_favor = []\n",
    "t_values_none = []\n",
    "t_values_against = []\n",
    "\n",
    "#antivaxxesrs\n",
    "#t_values_favor = [] \n",
    "#t_values_against = []\n",
    "#t_values_none = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b420d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/datasets/provaxxers/rotulos/favor.json', 'w') as file:\n",
    "     for t in t_values_favor:\n",
    "        file.write(json.dumps(topic_docs[t])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58620f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/datasets/provaxxers/rotulos/none.json', 'w') as file:\n",
    "     for t in t_values_none:\n",
    "        file.write(json.dumps(topic_docs[t])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b85e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/datasets/provaxxers/rotulos/against.json', 'w') as file:\n",
    "     for t in t_values_against:\n",
    "        file.write(json.dumps(topic_docs[t])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd623986",
   "metadata": {},
   "source": [
    "### 3.14- Visualize topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e928d",
   "metadata": {
    "id": "1f7e928d"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b39ab",
   "metadata": {
    "id": "1e2b39ab"
   },
   "source": [
    "### 3.15- Reducing the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df7dce",
   "metadata": {
    "id": "31df7dce"
   },
   "outputs": [],
   "source": [
    "newTopics, newProbs = topic_model.reduce_topics(docs.text, topics, probs, nr_topics=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392913af",
   "metadata": {
    "id": "392913af"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa33064",
   "metadata": {
    "id": "8fa33064"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264570f",
   "metadata": {
    "id": "1264570f"
   },
   "outputs": [],
   "source": [
    "topic_model.get_representative_docs(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073e5ab",
   "metadata": {
    "id": "a073e5ab"
   },
   "source": [
    "### 3.16- Dynamic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848f875",
   "metadata": {
    "id": "6848f875"
   },
   "outputs": [],
   "source": [
    "timestamps = docs.created_at.to_list()\n",
    "tweets = docs.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f79ee",
   "metadata": {
    "id": "779f79ee"
   },
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(docs=tweets, \n",
    "                                                topics=newTopics,                                                                                           \n",
    "                                                timestamps=timestamps, \n",
    "                                                global_tuning=True,\n",
    "                                                evolution_tuning=True, \n",
    "                                                nr_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca25ac",
   "metadata": {
    "id": "45ca25ac"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=11)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sbbd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
