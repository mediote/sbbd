{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a45be2",
   "metadata": {
    "id": "b8a45be2"
   },
   "source": [
    "# 1-  Data crawling on TwitterAPI: Full-archive search \n",
    "\n",
    "Documentation: https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all \n",
    "\n",
    "Endpoint URL: https://api.twitter.com/2/tweets/search/all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e68fd6",
   "metadata": {
    "id": "54e68fd6"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import urllib.request as urllib2\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "auth_token = os.environ.get('AUTH_TOKEN')\n",
    "header = {'Authorization': 'Bearer ' + auth_token}\n",
    "\n",
    "class TwitterHook():\n",
    "\n",
    "    def __init__(self, query, header = None, start_time = None, end_time = None, max_results= None):\n",
    "        self.query = query\n",
    "        self.header = header\n",
    "        self.start_time = '2020-02-29T00%3A00%3A00Z'\n",
    "        self.end_time = '2021-05-04T00%3A00%3A00Z'\n",
    "        self.max_results = '500'\n",
    "\n",
    "    def create_url(self):\n",
    "        query = self.query\n",
    "        start_time = self.start_time\n",
    "        end_time = self.end_time        \n",
    "        \n",
    "        tweet_fields = \"tweet.fields=author_id,id,created_at,in_reply_to_user_id,text\"\n",
    "        user_fields = \"expansions=author_id&user.fields=id,name,username,created_at\"\n",
    "        start_time = (\n",
    "            f\"&start_time={self.start_time}\"\n",
    "            if self.start_time\n",
    "            else \"\"\n",
    "        )\n",
    "        end_time = (\n",
    "            f\"&end_time={self.end_time}\"\n",
    "            if self.end_time\n",
    "            else \"\"\n",
    "        )\n",
    "        max_results  = (\n",
    "            f\"&max_results={self.max_results}\"\n",
    "            if self.max_results\n",
    "            else \"\"\n",
    "        )\n",
    "        url = \"https://api.twitter.com/2/tweets/search/all?query={}&{}&{}{}{}{}\".format(\n",
    "               query, tweet_fields, user_fields, start_time, end_time, max_results\n",
    "        )\n",
    "        return url\n",
    "\n",
    "    def connect_to_endpoint(self, url, header):\n",
    "        response = requests.get(url,headers=header)\n",
    "        listOfTweets = json.loads(response.content)\n",
    "        return  listOfTweets\n",
    "\n",
    "\n",
    "    def paginate(self, url, header, next_token=\"\"):\n",
    "        if next_token:\n",
    "            full_url = f\"{url}&next_token={next_token}\"\n",
    "            print('New Request on',full_url)\n",
    "        else:\n",
    "            full_url = url\n",
    "            print('New Request on',full_url)\n",
    "        data = self.connect_to_endpoint(full_url, header)\n",
    "        yield data\n",
    "        if \"next_token\" in data.get(\"meta\", {}):\n",
    "            yield from self.paginate(url, header, data['meta']['next_token'])\n",
    "\n",
    "\n",
    "    def run(self):  \n",
    "        url = self.create_url()\n",
    "        yield from self.paginate(url, header)\n",
    "        \n",
    "        \n",
    "def GetTweets(query):\n",
    "    tweets = pd.DataFrame()\n",
    "    for pg in TwitterHook(query).run():\n",
    "        time.sleep(1)  \n",
    "        \n",
    "        if 'data' in pg:\n",
    "            tweets =  tweets.append(pg['data'],ignore_index=True)\n",
    "        else:\n",
    "             print('Missing request')\n",
    "        \n",
    "    print('Done! Total of', len(tweets), 'tweets collected.')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a16896",
   "metadata": {
    "id": "61a16896",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = GetTweets(urllib2.quote('#vacinanao -rt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222cf05",
   "metadata": {
    "id": "a222cf05"
   },
   "source": [
    "# 2- Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66004b1c",
   "metadata": {},
   "source": [
    "### Import datasets from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aPg7p1mOUht",
   "metadata": {
    "id": "2aPg7p1mOUht"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id':\"1kzlbUH76yT_J4sMj8ZMf0ZsyrpNK0uhX\"}) \n",
    "downloaded.GetContentFile('provaxxersProcessed.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "provaxxers =  pd.read_csv('./provaxxersProcessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741672d",
   "metadata": {},
   "source": [
    "### Import datasets from local disck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47550598",
   "metadata": {
    "id": "47550598"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "antivaxxers = pd.read_csv('./datasets/antivaxxersTweets.csv', low_memory=False)\n",
    "provaxxers = pd.read_csv('./datasets/provaxxersTweets.csv', low_memory=False)\n",
    "\n",
    "start_date ='2020-03-11T00:59:59.000Z'\n",
    "end_date = '2022-04-06T00:00:00.000Z'\n",
    "\n",
    "mask_provaxxers = (provaxxers['created_at'] >= start_date) & (provaxxers['created_at'] <= end_date)\n",
    "provaxxers = provaxxers.loc[mask_provaxxers]\n",
    "\n",
    "mask_antivaxxers = (antivaxxers['created_at'] >= start_date) & (antivaxxers['created_at'] <= end_date)\n",
    "antivaxxers = antivaxxers.loc[mask_antivaxxers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8734080d",
   "metadata": {},
   "source": [
    "### Pre-proccess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae16ca83",
   "metadata": {
    "id": "ae16ca83"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "import unidecode\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "\n",
    "def proccess_text(tweets):\n",
    "    \n",
    "    # Removing links, mentions and hashtags\n",
    "    tweets['processed_text'] = tweets.text.str.replace(r'(http\\S+)', '',regex=True) \\\n",
    "                                          .str.replace(r'@[\\w]*', '',regex=True) \\\n",
    "                                          .str.replace(r'#[\\w]*','',regex=True) \n",
    "    print('[ok] - Removing links.')\n",
    "    print('[ok] - Removing mentions.')\n",
    "    print('[ok] - Removing hashtags.')\n",
    "\n",
    "    textWords = ' '.join([text for text in tweets.processed_text])\n",
    "\n",
    "    # Removing accent\n",
    "    textWords = [unidecode.unidecode(text) for text in tweets.processed_text ]    \n",
    "    print('[ok] - Removing accent.')\n",
    "    \n",
    "    # Creating a list of words and characters (stopwords) to be removed from the text\n",
    "    # stopWords = nltk.corpus.stopwords.words(\"portuguese\")    \n",
    "    print('[ok] - Creating a list of words and characters (stopwords) to be removed from the text.')\n",
    "    \n",
    "    \n",
    "    # Separating punctuation from words\n",
    "    punctSeparator = tokenize.WordPunctTokenizer()\n",
    "    punctuationList = list()\n",
    "    for punct in punctuation:\n",
    "        punctuationList.append(punct)\n",
    "        \n",
    "    #stopWords =   punctuationList + stopWords    \n",
    "    stopWords =   punctuationList\n",
    "    #print('[ok] - Separating punctuation from words.')\n",
    "\n",
    "\n",
    "    # Iterating over the text and removing stop words \n",
    "    trasnformedText = list()    \n",
    "    for text in textWords:\n",
    "        newText = list()   \n",
    "        text = text.lower()\n",
    "        textWords = punctSeparator.tokenize(text)\n",
    "        for words in textWords:\n",
    "             if words not in stopWords:\n",
    "                #newText.append(stemmer.stem(words))\n",
    "                newText.append(words)\n",
    "        trasnformedText.append(' '.join(newText))\n",
    "    tweets.processed_text = trasnformedText\n",
    "    print('[ok] - Removing punctuation and set text to lowecase.')\n",
    "   \n",
    "    # Removing all non-text characters\n",
    "    tweets.processed_text = tweets['processed_text'].str.replace(r\"[^a-zA-Z#]\", \" \", regex=True)                                                         \n",
    "    print('[ok] - Removing all non-text characters.')\n",
    "   \n",
    "    trasnformedText = list()\n",
    "    for phrase in tweets.processed_text:\n",
    "        newPhrase = list()   \n",
    "        newPhrase.append(' '.join(phrase.split()))\n",
    "        for words in newPhrase:\n",
    "            trasnformedText.append(''.join(newPhrase))\n",
    "    tweets.processed_text = trasnformedText\n",
    "    \n",
    "    # Removing tweets with less than three terms\n",
    "    index=[x for x in tweets.index if tweets.processed_text[x].count(' ') < 3]\n",
    "    tweets = tweets.drop(index)\n",
    "    print('[ok] - Removing tweets with less than three terms.')\n",
    "\n",
    "    # Removing empty lines\n",
    "    removeEmpty  = tweets.processed_text != ' '\n",
    "    tweets = tweets[removeEmpty]\n",
    "    print('[ok] - Removing empty lines.')\n",
    "\n",
    "    tweets.reset_index(inplace=True)\n",
    "    tweets = {'created_at': tweets.created_at, 'id':tweets.id,'author_id':tweets.author_id,'in_reply_to_user_id':tweets.in_reply_to_user_id, 'text': tweets.processed_text}\n",
    "    #tweets = {'text': tweets.processed_text,'stance':tweets.stance}\n",
    "    tweets = pd.DataFrame(tweets)\n",
    "    tweets = tweets.sort_values(['created_at']).reset_index().drop(columns=[\"index\"])\n",
    "    #tweets = tweets.reset_index().drop(columns=[\"index\"])\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c759d176",
   "metadata": {
    "id": "c759d176"
   },
   "outputs": [],
   "source": [
    "provaxxers = proccess_text(provaxxers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2cf6c",
   "metadata": {
    "id": "3bd2cf6c"
   },
   "source": [
    "# 3- Topic Modeling with BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6509094",
   "metadata": {
    "id": "e6509094"
   },
   "source": [
    "### Checking GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968c368",
   "metadata": {
    "id": "5968c368"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():        \n",
    "    device = torch.device(\"cuda\")    \n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c68ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757b7ac",
   "metadata": {},
   "source": [
    "### Checking RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f160a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740262a5",
   "metadata": {},
   "source": [
    "### Custom BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb74a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58202eb4",
   "metadata": {},
   "source": [
    "### Custom UMAP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095cc7cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 1.20 or less",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21732/1061778123.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mumap\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUMAP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TOKENIZERS_PARALLELISM\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"false\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mumap_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUMAP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_dist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cosine'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\umap\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch_warnings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimplefilter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mumap_\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUMAP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\umap\\umap_.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtril\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_tril\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtriu\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_triu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistances\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numba\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[0m_ensure_llvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m \u001b[0m_ensure_critical_deps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;31m# we know llvmlite is working as the above tests passed, import it now as SVML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numba\\__init__.py\u001b[0m in \u001b[0;36m_ensure_critical_deps\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Numba needs NumPy 1.17 or greater\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mnumpy_version\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Numba needs NumPy 1.20 or less\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Numba needs NumPy 1.20 or less"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=10, min_dist=0.0, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba33ee73",
   "metadata": {},
   "source": [
    "### Custom HDBSCAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=10, min_dist=0.0, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf5070",
   "metadata": {},
   "source": [
    "### Custom vectorizer model model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8e376",
   "metadata": {
    "id": "cda8e376"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(2, 2), stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c58db",
   "metadata": {},
   "source": [
    "### Initializing BERTopic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a980c25",
   "metadata": {
    "id": "9a980c25"
   },
   "outputs": [],
   "source": [
    "docs = trump_tweets\n",
    "\n",
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic(#language = 'english',\n",
    "                       embedding_model=bert_model,\n",
    "                       top_n_words=10,\n",
    "                       #n_gram_range=(1, 2),\n",
    "                       min_topic_size=50,   \n",
    "                       nr_topics = 'auto',\n",
    "                       umap_model=umap_model,  \n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       low_memory=True,\n",
    "                       calculate_probabilities=False, \n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4799676e",
   "metadata": {
    "id": "4799676e"
   },
   "source": [
    "### Generating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5825bfd9",
   "metadata": {
    "id": "5825bfd9"
   },
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(docs.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e2903",
   "metadata": {},
   "source": [
    "### Serialize models, topics and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4999633",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(\"./models/trump_all_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66581c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_topics = pd.DataFrame(topics)\n",
    "trump_topics.to_csv('./models/trump_all_topics.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs = {topic: [] for topic in set(topics)}\n",
    "for topic, doc in zip(topics, docs.text):\n",
    "    topic_docs[topic].append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44699101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "t_values_favor = [65,10,49,120,97,18,12]\n",
    "t_values_against = [86,37,238,22,34,98,4]\n",
    "t_values_none = [-1]\n",
    "\n",
    "with open('./datasets/trump/favor_trump.json', 'w') as file:\n",
    "     for t in t_values_favor:\n",
    "        file.write(json.dumps(topic_docs[t])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c46f3b",
   "metadata": {},
   "source": [
    "### Load models,topics and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885ba24",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic.load(\"./models/provaxxers/provaxxers_all_model\")\n",
    "#topics = topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd623986",
   "metadata": {},
   "source": [
    "### Visualize topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e928d",
   "metadata": {
    "id": "1f7e928d"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c143ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f147bfe",
   "metadata": {
    "id": "4f147bfe"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b39ab",
   "metadata": {
    "id": "1e2b39ab"
   },
   "source": [
    "### Reducing the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df7dce",
   "metadata": {
    "id": "31df7dce"
   },
   "outputs": [],
   "source": [
    "newTopics, newProbs = topic_model.reduce_topics(docs.text, topics, probs, nr_topics=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392913af",
   "metadata": {
    "id": "392913af"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa33064",
   "metadata": {
    "id": "8fa33064"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264570f",
   "metadata": {
    "id": "1264570f"
   },
   "outputs": [],
   "source": [
    "topic_model.get_representative_docs(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073e5ab",
   "metadata": {
    "id": "a073e5ab"
   },
   "source": [
    "### Dynamic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848f875",
   "metadata": {
    "id": "6848f875"
   },
   "outputs": [],
   "source": [
    "timestamps = docs.created_at.to_list()\n",
    "tweets = docs.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f79ee",
   "metadata": {
    "id": "779f79ee"
   },
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(docs=tweets, \n",
    "                                                topics=newTopics,                                                                                           \n",
    "                                                timestamps=timestamps, \n",
    "                                                global_tuning=True,\n",
    "                                                evolution_tuning=True, \n",
    "                                                nr_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca25ac",
   "metadata": {
    "id": "45ca25ac"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f568f0",
   "metadata": {
    "id": "75f568f0"
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sbbd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
