{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb1098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "from umap import UMAP\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, STATUS_OK, space_eval, Trials\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a115ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs =  pd.read_csv('./datasets/antivaxxers/antivaxxers_processed.csv')\n",
    "docs =  pd.read_csv('./datasets/provaxxers/provaxxers_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f711b1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-11T01:40:07.000Z</td>\n",
       "      <td>1237553858320031744</td>\n",
       "      <td>2276964030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>attention people if we believe the is as bad a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-11T01:42:55.000Z</td>\n",
       "      <td>1237554565580382209</td>\n",
       "      <td>1129184099070074880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lest we forget except when they don t and here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-11T02:00:12.000Z</td>\n",
       "      <td>1237558915090374656</td>\n",
       "      <td>16846937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in global citizens convinced to help protect c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-11T02:48:00.000Z</td>\n",
       "      <td>1237570942097907712</td>\n",
       "      <td>88914175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just published maike morrison and coworkers st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-11T02:50:44.000Z</td>\n",
       "      <td>1237571629313437696</td>\n",
       "      <td>1187401190</td>\n",
       "      <td>8.108359e+17</td>\n",
       "      <td>so many irl who aren t on twitter currently mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673801</th>\n",
       "      <td>2022-04-05T22:59:33.000Z</td>\n",
       "      <td>1511478678093541385</td>\n",
       "      <td>376817911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s typhoid conjugate vaccine tcv campaign kicks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673802</th>\n",
       "      <td>2022-04-05T23:01:17.000Z</td>\n",
       "      <td>1511479115496439812</td>\n",
       "      <td>310403500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>when it came to vaccinating our son against co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673803</th>\n",
       "      <td>2022-04-05T23:54:47.000Z</td>\n",
       "      <td>1511492578272854016</td>\n",
       "      <td>1389743442738229251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vax misinformation debunk by immunology prof d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673804</th>\n",
       "      <td>2022-04-05T23:54:47.000Z</td>\n",
       "      <td>1511492578272854016</td>\n",
       "      <td>1389743442738229251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vax misinformation debunk by immunology prof d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673805</th>\n",
       "      <td>2022-04-05T23:59:49.000Z</td>\n",
       "      <td>1511493844872966149</td>\n",
       "      <td>3249242963</td>\n",
       "      <td>9.772048e+08</td>\n",
       "      <td>it is unreal how the governments are simply ab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>673806 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      created_at                   id            author_id  \\\n",
       "0       2020-03-11T01:40:07.000Z  1237553858320031744           2276964030   \n",
       "1       2020-03-11T01:42:55.000Z  1237554565580382209  1129184099070074880   \n",
       "2       2020-03-11T02:00:12.000Z  1237558915090374656             16846937   \n",
       "3       2020-03-11T02:48:00.000Z  1237570942097907712             88914175   \n",
       "4       2020-03-11T02:50:44.000Z  1237571629313437696           1187401190   \n",
       "...                          ...                  ...                  ...   \n",
       "673801  2022-04-05T22:59:33.000Z  1511478678093541385            376817911   \n",
       "673802  2022-04-05T23:01:17.000Z  1511479115496439812            310403500   \n",
       "673803  2022-04-05T23:54:47.000Z  1511492578272854016  1389743442738229251   \n",
       "673804  2022-04-05T23:54:47.000Z  1511492578272854016  1389743442738229251   \n",
       "673805  2022-04-05T23:59:49.000Z  1511493844872966149           3249242963   \n",
       "\n",
       "        in_reply_to_user_id                                               text  \n",
       "0                       NaN  attention people if we believe the is as bad a...  \n",
       "1                       NaN  lest we forget except when they don t and here...  \n",
       "2                       NaN  in global citizens convinced to help protect c...  \n",
       "3                       NaN  just published maike morrison and coworkers st...  \n",
       "4              8.108359e+17  so many irl who aren t on twitter currently mu...  \n",
       "...                     ...                                                ...  \n",
       "673801                  NaN  s typhoid conjugate vaccine tcv campaign kicks...  \n",
       "673802                  NaN  when it came to vaccinating our son against co...  \n",
       "673803                  NaN  vax misinformation debunk by immunology prof d...  \n",
       "673804                  NaN  vax misinformation debunk by immunology prof d...  \n",
       "673805         9.772048e+08  it is unreal how the governments are simply ab...  \n",
       "\n",
       "[673806 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "552ae003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2060 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():        \n",
    "    device = torch.device(\"cuda\")    \n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d602909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89cc69e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45d594336c0417bb370f508da5fb58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21057 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = model.encode(docs.text, show_progress_bar=True,convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c16ed596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clusters(embeddings,\n",
    "                      n_neighbors,\n",
    "                      n_components, \n",
    "                      min_cluster_size,\n",
    "                      random_state = None):\n",
    "    \"\"\"\n",
    "    Generate HDBSCAN cluster object after reducing embedding dimensionality with UMAP\n",
    "    \"\"\"\n",
    "    \n",
    "    umap_embeddings = (UMAP(n_neighbors=n_neighbors, \n",
    "                                n_components=n_components, \n",
    "                                metric='cosine', \n",
    "                                random_state=random_state)\n",
    "                                .fit_transform(embeddings))\n",
    "\n",
    "    clusters = hdbscan.HDBSCAN(min_cluster_size = min_cluster_size,\n",
    "                               metric='euclidean', \n",
    "                               cluster_selection_method='eom').fit(umap_embeddings)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf9b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_clusters(clusters, prob_threshold = 0.05):\n",
    "    \"\"\"\n",
    "    Returns the label count and cost of a given cluster supplied from running hdbscan\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_labels = clusters.labels_\n",
    "    label_count = len(np.unique(cluster_labels))\n",
    "    total_num = len(clusters.labels_)\n",
    "    cost = (np.count_nonzero(clusters.probabilities_ < prob_threshold)/total_num)\n",
    "    \n",
    "    return label_count, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729c2b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params, embeddings, label_lower, label_upper):\n",
    "    \"\"\"\n",
    "    Objective function for hyperopt to minimize, which incorporates constraints\n",
    "    on the number of clusters we want to identify\n",
    "    \"\"\"\n",
    "    \n",
    "    clusters = generate_clusters(embeddings, \n",
    "                                 n_neighbors = params['n_neighbors'], \n",
    "                                 n_components = params['n_components'], \n",
    "                                 min_cluster_size = params['min_cluster_size'],\n",
    "                                 random_state = params['random_state'])\n",
    "    \n",
    "    label_count, cost = score_clusters(clusters, prob_threshold = 0.05)\n",
    "    \n",
    "    #15% penalty on the cost function if outside the desired range of groups\n",
    "    if (label_count < label_lower) | (label_count > label_upper):\n",
    "        penalty = 0.15 \n",
    "    else:\n",
    "        penalty = 0\n",
    "    \n",
    "    loss = cost + penalty\n",
    "    \n",
    "    return {'loss': loss, 'label_count': label_count, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5926b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_search(embeddings, space, label_lower, label_upper, max_evals=100):\n",
    "    \"\"\"\n",
    "    Perform bayseian search on hyperopt hyperparameter space to minimize objective function\n",
    "    \"\"\"\n",
    "    \n",
    "    trials = Trials()\n",
    "    fmin_objective = partial(objective, embeddings=embeddings, label_lower=label_lower, label_upper=label_upper)\n",
    "    best = fmin(fmin_objective,  \n",
    "                space = space, \n",
    "                algo=tpe.suggest,\n",
    "                max_evals=max_evals, \n",
    "                trials=trials)\n",
    "\n",
    "    best_params = space_eval(space, best)\n",
    "    print ('best:')\n",
    "    print (best_params)\n",
    "    print (f\"label count: {trials.best_trial['result']['label_count']}\")\n",
    "    \n",
    "    best_clusters = generate_clusters(embeddings, \n",
    "                                      n_neighbors = best_params['n_neighbors'], \n",
    "                                      n_components = best_params['n_components'], \n",
    "                                      min_cluster_size = best_params['min_cluster_size'],\n",
    "                                      random_state = best_params['random_state'])\n",
    "    \n",
    "    return best_params, best_clusters, trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a7d3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hspace = {\n",
    "    'n_neighbors': hp.choice('n_neighbors',range(12,16)),\n",
    "    'n_components': hp.choice('n_components',range(3,8)),\n",
    "    'min_cluster_size': hp.choice('min_cluster_size',range(35,100)),\n",
    "    'random_state':42\n",
    "}\n",
    "\n",
    "label_lower=500\n",
    "label_upper=1000\n",
    "max_evals = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae143ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inf\\Anaconda3\\lib\\site-packages\\umap\\spectral.py:260: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 3/100 [8:27:04<277:17:43, 10291.37s/trial, best loss: 0.6165201853352449]"
     ]
    }
   ],
   "source": [
    "best_params, best_clusters, trials = bayesian_search(embeddings,\n",
    "                                                     space=hspace,\n",
    "                                                     label_lower=label_lower,\n",
    "                                                     label_upper=label_upper,\n",
    "                                                     max_evals=max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601af22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(best_clusters.probabilities_[best_clusters.probabilities_ == 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794797f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_documents(docs,cluster_labels):\n",
    "    docs_df = pd.DataFrame(docs, columns=[\"text\"])\n",
    "    docs_df['cluster'] = cluster_labels\n",
    "    docs_df['doc_id'] = range(len(docs_df))\n",
    "    docs_per_topic = docs_df.groupby(['cluster'], as_index = False).agg({'text': ' '.join})\n",
    "    return docs_df, docs_per_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fa7adc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df, docs_per_topic = clustering_documents(docs,best_clusters.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "214821c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words(\"portuguese\")   \n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(3, 3)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range,stop_words=stop_words ).fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "c_tf_idf, count = c_tf_idf(docs_per_topic.text.values, m=len(docs))\n",
    "\n",
    "def extract_top_n_words_per_topic(c_tf_idf, count, docs_per_topic, n=10):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.cluster)\n",
    "    tf_idf_transposed = c_tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return words, top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['cluster'])\n",
    "                     .text\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"cluster\": \"topic\", \"text\": \"size\"}, axis='columns')\n",
    "                     .sort_values(\"size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "words,top_n_words = extract_top_n_words_per_topic(c_tf_idf, count, docs_per_topic, n=5)\n",
    "topic_sizes = extract_topic_sizes(docs_df); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "430d7c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>7252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>38</td>\n",
       "      <td>3615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>45</td>\n",
       "      <td>3048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>37</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>35</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>46</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>41</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>36</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>39</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>31</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>33</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>42</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>44</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>40</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>43</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>34</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic  size\n",
       "0      -1  7252\n",
       "39     38  3615\n",
       "46     45  3048\n",
       "17     16   958\n",
       "38     37   618\n",
       "28     27   390\n",
       "36     35   333\n",
       "47     46   331\n",
       "12     11   322\n",
       "42     41   312\n",
       "31     30   285\n",
       "25     24   250\n",
       "37     36   213\n",
       "40     39   188\n",
       "27     26   183\n",
       "1       0   182\n",
       "32     31   166\n",
       "34     33   159\n",
       "19     18   158\n",
       "43     42   152\n",
       "4       3   151\n",
       "20     19   148\n",
       "45     44   143\n",
       "11     10   139\n",
       "29     28   131\n",
       "3       2   125\n",
       "22     21   119\n",
       "41     40   119\n",
       "33     32   115\n",
       "10      9   110\n",
       "15     14   100\n",
       "14     13    93\n",
       "8       7    92\n",
       "13     12    82\n",
       "24     23    80\n",
       "18     17    77\n",
       "2       1    76\n",
       "26     25    74\n",
       "30     29    73\n",
       "44     43    72\n",
       "9       8    71\n",
       "35     34    69\n",
       "16     15    65\n",
       "5       4    64\n",
       "21     20    64\n",
       "6       5    64\n",
       "23     22    56\n",
       "7       6    56"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7d744b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def  most_common(lst, n_words):\n",
    "        \"\"\"\n",
    "        Return most common n words in list of words\n",
    "        Arguments:\n",
    "            lst: list of words\n",
    "            n_words: int, number of top words by frequency to return\n",
    "        Returns:\n",
    "            counter.most_common(n_words): a list of the n most common elements\n",
    "                                          and their counts from the most\n",
    "                                          common to the least\n",
    "        \"\"\"\n",
    "\n",
    "        counter = collections.Counter(lst)\n",
    "\n",
    "        return counter.most_common(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4b828fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading language model for the spaCy dependency parser\n",
      "(only required the first time this is run)\n",
      "\n",
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "try:\n",
    "     #nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading language model for the spaCy dependency parser\\n\"\n",
    "                  \"(only required the first time this is run)\\n\")\n",
    "    from spacy.cli import download\n",
    "    #download(\"en_core_web_sm\")\n",
    "    download(\"pt_core_news_sm\")\n",
    "    #nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp =spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def extract_labels(category_docs):\n",
    "    \"\"\"\n",
    "    Extract labels from documents in the same cluster by concatenating\n",
    "    most common verbs, ojects, and nouns\n",
    "    \"\"\"\n",
    "\n",
    "    verbs = []\n",
    "    dobjs = []\n",
    "    nouns = []\n",
    "    adjs = []\n",
    "    \n",
    "    verb = ''\n",
    "    dobj = ''\n",
    "    noun1 = ''\n",
    "    noun2 = ''\n",
    "\n",
    "    # for each document, append verbs, dobs, nouns, and adjectives to \n",
    "    # running lists for whole cluster\n",
    "    for i in range(len(category_docs)):\n",
    "        doc = nlp(category_docs[i])\n",
    "        for token in doc:\n",
    "            if token.is_stop==False:\n",
    "                if token.dep_ == 'ROOT':\n",
    "                    verbs.append(token.text.lower())\n",
    "\n",
    "                elif token.dep_=='dobj':\n",
    "                    dobjs.append(token.lemma_.lower())\n",
    "\n",
    "                elif token.pos_=='NOUN':\n",
    "                    nouns.append(token.lemma_.lower())\n",
    "                    \n",
    "                elif token.pos_=='ADJ':\n",
    "                    adjs.append(token.lemma_.lower())\n",
    "    \n",
    "    # take most common words of each form\n",
    "    if len(verbs) > 0:\n",
    "        verb = most_common(verbs, 1)[0][0]\n",
    "    \n",
    "    if len(dobjs) > 0:\n",
    "        dobj = most_common(dobjs, 1)[0][0]\n",
    "    \n",
    "    if len(nouns) > 0:\n",
    "        noun1 = most_common(nouns, 1)[0][0]\n",
    "    \n",
    "    if len(set(nouns)) > 1:\n",
    "        noun2 = most_common(nouns, 2)[1][0]\n",
    "    \n",
    "    # concatenate the most common verb-dobj-noun1-noun2 (if they exist)\n",
    "    label_words = [verb, dobj]\n",
    "    \n",
    "    for word in [noun1, noun2]:\n",
    "        if word not in label_words:\n",
    "            label_words.append(word)\n",
    "    \n",
    "    if '' in label_words:\n",
    "        label_words.remove('')\n",
    "    \n",
    "    label = '_'.join(label_words)\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9abd050e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10788/959831585.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcluster\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocs_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mlabel_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10788/950485538.py\u001b[0m in \u001b[0;36mextract_labels\u001b[1;34m(category_docs)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# running lists for whole cluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1015\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m                 \u001b[1;31m# This typically happens if a component is not initialized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.set_annotations\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\_parser_internals\\ner.pyx\u001b[0m in \u001b[0;36mspacy.pipeline._parser_internals.ner.BiluoPushDown.set_annotations\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.SetEntsDefault.values\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\enum.py\u001b[0m in \u001b[0;36m__members__\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_member_names_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__members__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \"\"\"\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cluster_labels = np.unique(best_clusters.labels_)\n",
    "\n",
    "label_dict = {}\n",
    "for label in cluster_labels:\n",
    "    cluster =  pd.DataFrame(docs_df[docs_df.cluster==label].text)\n",
    "    cluster = cluster.reset_index().drop(columns=[\"index\"])\n",
    "    label_dict[label] = extract_labels(cluster.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808629e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394930c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f7e449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98895806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce55a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "from typing import List\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def visualize_topics(topic_model,\n",
    "                     topics: List[int] = None,\n",
    "                     top_n_topics: int = None,\n",
    "                     width: int = 650,\n",
    "                     height: int = 650) -> go.Figure:\n",
    "    \"\"\" Visualize topics, their sizes, and their corresponding words\n",
    "    This visualization is highly inspired by LDAvis, a great visualization\n",
    "    technique typically reserved for LDA.\n",
    "    Arguments:\n",
    "        topic_model: A fitted BERTopic instance.\n",
    "        topics: A selection of topics to visualize\n",
    "        top_n_topics: Only select the top n most frequent topics\n",
    "        width: The width of the figure.\n",
    "        height: The height of the figure.\n",
    "    Usage:\n",
    "    To visualize the topics simply run:\n",
    "    ```python\n",
    "    topic_model.visualize_topics()\n",
    "    ```\n",
    "    Or if you want to save the resulting figure:\n",
    "    ```python\n",
    "    fig = topic_model.visualize_topics()\n",
    "    fig.write_html(\"path/to/file.html\")\n",
    "    ```\n",
    "    <iframe src=\"../../getting_started/visualization/viz.html\"\n",
    "    style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe>\n",
    "    \"\"\"\n",
    "    # Select topics based on top_n and topics args\n",
    "    if topics is not None:\n",
    "        topics = list(topics)\n",
    "    elif top_n_topics is not None:\n",
    "        topics = sorted(topic_model.get_topic_freq().Topic.to_list()[1:top_n_topics + 1])\n",
    "    else:\n",
    "        topics = sorted(list(topic_model.get_topics().keys()))\n",
    "\n",
    "    # Extract topic words and their frequencies\n",
    "    topic_list = sorted(topics)\n",
    "    frequencies = [topic_model.topic_sizes[topic] for topic in topic_list]\n",
    "    words = [\" | \".join([word[0] for word in topic_model.get_topic(topic)[:5]]) for topic in topic_list]\n",
    "\n",
    "    # Embed c-TF-IDF into 2D\n",
    "    all_topics = sorted(list(topic_model.get_topics().keys()))\n",
    "    indices = np.array([all_topics.index(topic) for topic in topics])\n",
    "    embeddings = topic_model.c_tf_idf.toarray()[indices]\n",
    "    embeddings = MinMaxScaler().fit_transform(embeddings)\n",
    "    embeddings = UMAP(n_neighbors=2, n_components=2, metric='hellinger').fit_transform(embeddings)\n",
    "\n",
    "    # Visualize with plotly\n",
    "    df = pd.DataFrame({\"x\": embeddings[1:, 0], \"y\": embeddings[1:, 1],\n",
    "                       \"Topic\": topic_list[1:], \"Words\": words[1:], \"Size\": frequencies[1:]})\n",
    "    return _plotly_topic_visualization(df, topic_list, width, height)\n",
    "\n",
    "\n",
    "def _plotly_topic_visualization(df: pd.DataFrame,\n",
    "                                topic_list: List[str],\n",
    "                                width: int,\n",
    "                                height: int):\n",
    "    \"\"\" Create plotly-based visualization of topics with a slider for topic selection \"\"\"\n",
    "\n",
    "    def get_color(topic_selected):\n",
    "        if topic_selected == -1:\n",
    "            marker_color = [\"#B0BEC5\" for _ in topic_list[1:]]\n",
    "        else:\n",
    "            marker_color = [\"red\" if topic == topic_selected else \"#B0BEC5\" for topic in topic_list[1:]]\n",
    "        return [{'marker.color': [marker_color]}]\n",
    "\n",
    "    # Prepare figure range\n",
    "    x_range = (df.x.min() - abs((df.x.min()) * .15), df.x.max() + abs((df.x.max()) * .15))\n",
    "    y_range = (df.y.min() - abs((df.y.min()) * .15), df.y.max() + abs((df.y.max()) * .15))\n",
    "\n",
    "    # Plot topics\n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", size=\"Size\", size_max=40, template=\"simple_white\", labels={\"x\": \"\", \"y\": \"\"},\n",
    "                     hover_data={\"Topic\": True, \"Words\": True, \"Size\": True, \"x\": False, \"y\": False})\n",
    "    fig.update_traces(marker=dict(color=\"#B0BEC5\", line=dict(width=2, color='DarkSlateGrey')))\n",
    "\n",
    "    # Update hover order\n",
    "    fig.update_traces(hovertemplate=\"<br>\".join([\"<b>Topic %{customdata[0]}</b>\",\n",
    "                                                 \"Words: %{customdata[1]}\",\n",
    "                                                 \"Size: %{customdata[2]}\"]))\n",
    "\n",
    "    # Create a slider for topic selection\n",
    "    steps = [dict(label=f\"Topic {topic}\", method=\"update\", args=get_color(topic)) for topic in topic_list[1:]]\n",
    "    sliders = [dict(active=0, pad={\"t\": 50}, steps=steps)]\n",
    "\n",
    "    # Stylize layout\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': \"<b>Intertopic Distance Map\",\n",
    "            'y': .95,\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top',\n",
    "            'font': dict(\n",
    "                size=22,\n",
    "                color=\"Black\")\n",
    "        },\n",
    "        width=width,\n",
    "        height=height,\n",
    "        hoverlabel=dict(\n",
    "            bgcolor=\"white\",\n",
    "            font_size=16,\n",
    "            font_family=\"Rockwell\"\n",
    "        ),\n",
    "        xaxis={\"visible\": False},\n",
    "        yaxis={\"visible\": False},\n",
    "        sliders=sliders\n",
    "    )\n",
    "\n",
    "    # Update axes ranges\n",
    "    fig.update_xaxes(range=x_range)\n",
    "    fig.update_yaxes(range=y_range)\n",
    "\n",
    "    # Add grid in a 'plus' shape\n",
    "    fig.add_shape(type=\"line\",\n",
    "                  x0=sum(x_range) / 2, y0=y_range[0], x1=sum(x_range) / 2, y1=y_range[1],\n",
    "                  line=dict(color=\"#CFD8DC\", width=2))\n",
    "    fig.add_shape(type=\"line\",\n",
    "                  x0=x_range[0], y0=sum(y_range) / 2, x1=x_range[1], y1=sum(y_range) / 2,\n",
    "                  line=dict(color=\"#9E9E9E\", width=2))\n",
    "    fig.add_annotation(x=x_range[0], y=sum(y_range) / 2, text=\"D1\", showarrow=False, yshift=10)\n",
    "    fig.add_annotation(y=y_range[1], x=sum(x_range) / 2, text=\"D2\", showarrow=False, xshift=10)\n",
    "    fig.data = fig.data[::-1]\n",
    "\n",
    "    return fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
